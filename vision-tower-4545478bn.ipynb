{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch transformers onnx onnxruntime pillow accelerate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install onnxscript","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers==4.57.6","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nExport MedGemma Vision Encoder to ONNX - Direct Export Version\n================================================================\n\nThis version exports the vision encoder directly without reloading weights.\nSimpler and more reliable.\n\nColab Setup:\n```python\n# Cell 1: Install dependencies  \n!pip install torch transformers onnx onnxruntime pillow accelerate\n\n# Cell 2: Run export\n%run export_vision_onnx.py\n```\n\"\"\"\n\nimport os\nimport gc\nimport json\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n\ndef clear_memory():\n    \"\"\"Clear GPU and CPU memory.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\nclass VisionEncoderWrapper(nn.Module):\n    \"\"\"\n    Simple wrapper around the vision tower for ONNX export.\n    Does NOT copy weights - uses the original vision tower directly.\n    \"\"\"\n    \n    def __init__(self, vision_tower, projection_weight):\n        super().__init__()\n        self.vision_tower = vision_tower\n        self.register_buffer('projection_weight', projection_weight.clone().detach())\n    \n    def forward(self, pixel_values):\n        # Get vision embeddings from SigLIP\n        vision_outputs = self.vision_tower(pixel_values)\n        \n        if hasattr(vision_outputs, 'last_hidden_state'):\n            vision_embeds = vision_outputs.last_hidden_state\n        else:\n            vision_embeds = vision_outputs[0]\n        \n        batch_size = vision_embeds.shape[0]\n        num_patches = vision_embeds.shape[1]\n        hidden_dim = vision_embeds.shape[2]\n        \n        # Pool to 256 patches if needed (for 16x16 grid)\n        if num_patches > 256:\n            side = int(num_patches ** 0.5)\n            pool_factor = side // 16\n            vision_embeds = vision_embeds.reshape(batch_size, side, side, hidden_dim)\n            vision_embeds = vision_embeds.reshape(batch_size, 16, pool_factor, 16, pool_factor, hidden_dim)\n            vision_embeds = vision_embeds.mean(dim=(2, 4))\n            vision_embeds = vision_embeds.reshape(batch_size, 256, hidden_dim)\n        \n        # Project to LLM embedding space\n        projected = torch.matmul(vision_embeds, self.projection_weight)\n        \n        # Attention scores = projection magnitude (normalized)\n        attention = torch.norm(projected, dim=-1)\n        att_min = attention.min(dim=-1, keepdim=True)[0]\n        att_max = attention.max(dim=-1, keepdim=True)[0]\n        attention = (attention - att_min) / (att_max - att_min + 1e-8)\n        \n        # Global pooled representation\n        pooled = projected.mean(dim=1)\n        \n        return projected, attention, pooled\n\n\ndef main():\n    \"\"\"Main export function - direct approach.\"\"\"\n    from transformers import AutoModelForImageTextToText, AutoProcessor\n    \n    output_dir = \"./onnx_export\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"=\"*60)\n    print(\"MedGemma Vision Encoder ONNX Export\")\n    print(\"Direct Export Version (No Weight Reload)\")\n    print(\"=\"*60)\n    \n    # ========================================\n    # Step 1: Load model WITHOUT quantization\n    # ========================================\n    print(\"\\n[Step 1] Loading model in FP16 (no quantization)...\")\n    \n    model_id = \"convaiinnovations/medgemma-4b-ecginstruct\"\n    \n    # Load in FP16 without quantization\n    # This uses more memory but gives us proper weights\n    model = AutoModelForImageTextToText.from_pretrained(\n        model_id,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        low_cpu_mem_usage=True,\n    )\n    processor = AutoProcessor.from_pretrained(model_id)\n    \n    print(f\"Model loaded on {model.device}\")\n    \n    # ========================================\n    # Step 2: Extract vision components\n    # ========================================\n    print(\"\\n[Step 2] Extracting vision encoder...\")\n    \n    vision_tower = model.vision_tower\n    projector = model.multi_modal_projector\n    \n    # Get projection weight\n    if hasattr(projector, 'mm_input_projection_weight'):\n        proj_weight = projector.mm_input_projection_weight.data.float()\n    else:\n        for module in projector.modules():\n            if isinstance(module, nn.Linear):\n                proj_weight = module.weight.data.float()\n                break\n    \n    print(f\"Vision tower: {type(vision_tower).__name__}\")\n    print(f\"Projection weight: {proj_weight.shape}\")\n    \n    # Save projection weight for edge CMAS\n    np.save(os.path.join(output_dir, \"projection_weight.npy\"), proj_weight.cpu().numpy())\n    \n    # Get image size\n    if hasattr(processor, 'image_processor'):\n        img_size = processor.image_processor.size\n        height = img_size.get('height', 896) if isinstance(img_size, dict) else img_size\n        width = img_size.get('width', 896) if isinstance(img_size, dict) else img_size\n    else:\n        height = width = 896\n    \n    # ========================================\n    # Step 3: Create wrapper and move to CPU\n    # ========================================\n    print(\"\\n[Step 3] Creating export wrapper...\")\n    \n    # Remove accelerate hooks before moving to CPU\n    from accelerate.hooks import remove_hook_from_module\n    \n    def remove_all_hooks(module):\n        \"\"\"Recursively remove accelerate hooks from all submodules.\"\"\"\n        for name, child in module.named_children():\n            remove_all_hooks(child)\n        if hasattr(module, '_hf_hook'):\n            remove_hook_from_module(module)\n    \n    remove_all_hooks(vision_tower)\n    print(\"Accelerate hooks removed.\")\n    \n    # Move vision tower to CPU and float32 for ONNX export\n    vision_tower = vision_tower.cpu().float()\n    proj_weight = proj_weight.cpu()\n    \n    # Delete full model to free GPU memory\n    del model\n    del projector\n    clear_memory()\n    print(\"GPU memory freed.\")\n    \n    # Create wrapper\n    wrapper = VisionEncoderWrapper(vision_tower, proj_weight)\n    wrapper.eval()\n    \n    # ========================================\n    # Step 4: Test forward pass\n    # ========================================\n    print(\"\\n[Step 4] Testing forward pass...\")\n    \n    # Ensure dummy input is on CPU (same device as model)\n    dummy_input = torch.randn(1, 3, height, width, device='cpu')\n    \n    with torch.no_grad():\n        projected, attention, pooled = wrapper(dummy_input)\n    \n    print(f\"  Projected: {projected.shape}\")\n    print(f\"  Attention: {attention.shape}\")  \n    print(f\"  Pooled: {pooled.shape}\")\n    \n    # ========================================\n    # Step 5: Export to ONNX\n    # ========================================\n    print(\"\\n[Step 5] Exporting to ONNX...\")\n    \n    onnx_path = os.path.join(output_dir, \"vision_encoder.onnx\")\n    \n    # Use legacy ONNX export (more stable)\n    torch.onnx.export(\n        wrapper,\n        dummy_input,\n        onnx_path,\n        export_params=True,\n        opset_version=18,  # Use opset 18 to match PyTorch's implementations\n        do_constant_folding=True,\n        input_names=['pixel_values'],\n        output_names=['projected_embeddings', 'attention_scores', 'pooled_embedding'],\n        dynamic_axes={\n            'pixel_values': {0: 'batch_size'},\n            'projected_embeddings': {0: 'batch_size'},\n            'attention_scores': {0: 'batch_size'},\n            'pooled_embedding': {0: 'batch_size'}\n        },\n        dynamo=False  # Use legacy exporter for stability\n    )\n    \n    file_size = os.path.getsize(onnx_path) / (1024 * 1024)\n    print(f\"ONNX exported: {file_size:.1f} MB\")\n    \n    # ========================================\n    # Step 6: Verify\n    # ========================================\n    print(\"\\n[Step 6] Verifying ONNX model...\")\n    \n    import onnx\n    import onnxruntime as ort\n    \n    onnx_model = onnx.load(onnx_path)\n    onnx.checker.check_model(onnx_model)\n    print(\"  ONNX validation: ✅\")\n    \n    session = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n    test_input = np.random.randn(1, 3, height, width).astype(np.float32)\n    outputs = session.run(None, {'pixel_values': test_input})\n    print(f\"  Inference test: ✅\")\n    print(f\"    - Projected: {outputs[0].shape}\")\n    print(f\"    - Attention: {outputs[1].shape}\")\n    print(f\"    - Pooled: {outputs[2].shape}\")\n    \n    # ========================================\n    # Save config\n    # ========================================\n    config = {\n        'input_height': height,\n        'input_width': width,\n        'num_patches': 256,\n        'grid_size': 16,\n        'projected_dim': int(proj_weight.shape[1]),\n        'model_id': model_id,\n    }\n    \n    with open(os.path.join(output_dir, \"vision_config.json\"), 'w') as f:\n        json.dump(config, f, indent=2)\n    \n    # ========================================\n    # Done!\n    # ========================================\n    print(\"\\n\" + \"=\"*60)\n    print(\"✅ EXPORT COMPLETE!\")\n    print(\"=\"*60)\n    print(f\"\"\"\nFiles in {output_dir}/:\n  • vision_encoder.onnx    ({file_size:.1f} MB)\n  • vision_config.json\n  • projection_weight.npy\n\nDownload and use with edge_inference.py on your desktop.\n\"\"\")\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile edge_inference.py\n\"\"\"\nEdge Inference with ONNX Vision Encoder\n=========================================\n\nUse this on the desktop app to compute attention/CMAS locally\nwithout needing the full MedGemma model.\n\nUsage:\n    from edge_inference import EdgeVisionEncoder\n    \n    encoder = EdgeVisionEncoder(\"./onnx_export\")\n    attention_map, heatmap_image = encoder.get_attention_heatmap(ecg_image)\n\"\"\"\n\nimport os\nimport json\nimport numpy as np\nfrom PIL import Image\n\n\nclass EdgeVisionEncoder:\n    \"\"\"\n    Local edge inference for attention/CMAS computation.\n    Uses ONNX runtime for efficient CPU/GPU inference.\n    \"\"\"\n    \n    def __init__(self, onnx_dir, use_gpu=False):\n        \"\"\"\n        Initialize the edge vision encoder.\n        \n        Args:\n            onnx_dir: Directory containing vision_encoder.onnx and vision_config.json\n            use_gpu: Whether to use GPU (requires onnxruntime-gpu)\n        \"\"\"\n        import onnxruntime as ort\n        \n        # Load config\n        config_path = os.path.join(onnx_dir, \"vision_config.json\")\n        with open(config_path, 'r') as f:\n            self.config = json.load(f)\n        \n        # Set up ONNX session\n        onnx_path = os.path.join(onnx_dir, \"vision_encoder.onnx\")\n        \n        if use_gpu:\n            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n        else:\n            providers = ['CPUExecutionProvider']\n        \n        self.session = ort.InferenceSession(onnx_path, providers=providers)\n        \n        # Load projection weight if available (for full CMAS)\n        proj_path = os.path.join(onnx_dir, \"projection_weight.npy\")\n        if os.path.exists(proj_path):\n            self.projection_weight = np.load(proj_path)\n        else:\n            self.projection_weight = None\n        \n        self.input_height = self.config.get('input_height', 896)\n        self.input_width = self.config.get('input_width', 896)\n        self.grid_size = self.config.get('grid_size', 16)\n        \n        print(f\"EdgeVisionEncoder initialized\")\n        print(f\"  Input size: {self.input_height}x{self.input_width}\")\n        print(f\"  Grid size: {self.grid_size}x{self.grid_size}\")\n    \n    def preprocess_image(self, image):\n        \"\"\"\n        Preprocess image for vision encoder.\n        \n        Args:\n            image: PIL Image or path to image\n            \n        Returns:\n            numpy array of shape [1, 3, H, W]\n        \"\"\"\n        if isinstance(image, str):\n            image = Image.open(image)\n        \n        if image.mode != 'RGB':\n            image = image.convert('RGB')\n        \n        # Resize to model input size\n        image = image.resize((self.input_width, self.input_height), Image.BILINEAR)\n        \n        # Convert to numpy and normalize to [0, 1]\n        img_array = np.array(image).astype(np.float32) / 255.0\n        \n        # Normalize with ImageNet stats (standard for vision models)\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        img_array = (img_array - mean) / std\n        \n        # Transpose to [C, H, W] and add batch dimension\n        img_array = img_array.transpose(2, 0, 1)\n        img_array = np.expand_dims(img_array, 0).astype(np.float32)\n        \n        return img_array\n    \n    def compute_attention(self, image):\n        \"\"\"\n        Compute attention scores for an image.\n        \n        Args:\n            image: PIL Image or path to image\n            \n        Returns:\n            attention_2d: [16, 16] attention map\n            projected: [256, hidden_dim] projected embeddings\n            pooled: [hidden_dim] global representation\n        \"\"\"\n        # Preprocess\n        pixel_values = self.preprocess_image(image)\n        \n        # Run ONNX inference\n        outputs = self.session.run(None, {'pixel_values': pixel_values})\n        projected, attention, pooled = outputs\n        \n        # Reshape attention to 2D grid (16x16)\n        attention_2d = attention[0].reshape(self.grid_size, self.grid_size)\n        \n        return attention_2d, projected[0], pooled[0]\n    \n    def compute_cmas_with_reference(self, image, reference_embedding):\n        \"\"\"\n        Compute full CMAS using a reference diagnosis embedding.\n        \n        Args:\n            image: PIL Image or path to image\n            reference_embedding: [hidden_dim] reference embedding for comparison\n            \n        Returns:\n            cmas_2d: [16, 16] CMAS attention map\n        \"\"\"\n        attention_2d, projected, pooled = self.compute_attention(image)\n        \n        # Compute cosine similarity with reference\n        reference = np.array(reference_embedding)\n        \n        # Normalize\n        proj_norm = projected / (np.linalg.norm(projected, axis=-1, keepdims=True) + 1e-8)\n        ref_norm = reference / (np.linalg.norm(reference) + 1e-8)\n        \n        # Cosine similarity per patch\n        cos_sim = np.dot(proj_norm, ref_norm)  # [256]\n        \n        # Magnitude\n        magnitudes = np.linalg.norm(projected, axis=-1)  # [256]\n        \n        # CMAS = magnitude × cosine_similarity\n        cmas = magnitudes * cos_sim\n        \n        # Normalize to [0, 1]\n        cmas = cmas - cmas.min()\n        cmas = cmas / (cmas.max() + 1e-8)\n        \n        # Reshape to 16x16 grid\n        cmas_2d = cmas.reshape(self.grid_size, self.grid_size)\n        \n        return cmas_2d\n    \n    def get_attention_heatmap(self, image, alpha=0.5, colormap='jet', invert=True):\n        \"\"\"\n        Generate attention heatmap overlay on image.\n        \n        Args:\n            image: PIL Image or path to image\n            alpha: Overlay transparency (0-1)\n            colormap: Matplotlib colormap name ('jet', 'hot', 'viridis', etc.)\n            invert: If True, invert attention (highlight high-attention as red)\n            \n        Returns:\n            attention_2d: [16, 16] raw attention scores\n            heatmap_image: PIL Image with heatmap overlay\n        \"\"\"\n        # Load original image\n        if isinstance(image, str):\n            original_image = Image.open(image).convert('RGB')\n        else:\n            original_image = image.convert('RGB')\n        \n        # Compute attention\n        attention_2d, _, _ = self.compute_attention(original_image)\n        \n        # Optionally invert (so high attention = red)\n        if invert:\n            attention_2d = 1.0 - attention_2d\n        \n        # Generate heatmap overlay\n        heatmap_image = self._generate_heatmap_overlay(\n            original_image, attention_2d, alpha, colormap\n        )\n        \n        return attention_2d, heatmap_image\n    \n    def _generate_heatmap_overlay(self, image, attention_2d, alpha=0.5, colormap='jet'):\n        \"\"\"Generate heatmap overlay on image.\"\"\"\n        import matplotlib.cm as cm\n        \n        img_array = np.array(image)\n        h, w = img_array.shape[:2]\n        \n        # Resize attention to image size using bilinear interpolation\n        attention_resized = Image.fromarray((attention_2d * 255).astype(np.uint8))\n        attention_resized = attention_resized.resize((w, h), Image.BILINEAR)\n        attention_array = np.array(attention_resized) / 255.0\n        \n        # Apply colormap\n        cmap = cm.get_cmap(colormap)\n        heatmap_colored = cmap(attention_array)[:, :, :3]\n        heatmap_colored = (heatmap_colored * 255).astype(np.uint8)\n        \n        # Blend with original image\n        blended = (1 - alpha) * img_array + alpha * heatmap_colored\n        blended = blended.astype(np.uint8)\n        \n        return Image.fromarray(blended)\n    \n    def get_top_attention_regions(self, image, top_k=5):\n        \"\"\"\n        Get the top-k most attended regions.\n        \n        Args:\n            image: PIL Image or path to image\n            top_k: Number of top regions to return\n            \n        Returns:\n            List of (row, col, attention_score) tuples\n        \"\"\"\n        attention_2d, _, _ = self.compute_attention(image)\n        \n        # Flatten and get top-k indices\n        flat_attention = attention_2d.flatten()\n        top_indices = np.argsort(flat_attention)[-top_k:][::-1]\n        \n        regions = []\n        for idx in top_indices:\n            row = idx // self.grid_size\n            col = idx % self.grid_size\n            score = float(flat_attention[idx])\n            regions.append((row, col, score))\n        \n        return regions\n    \n    def get_pooled_embedding(self, image):\n        \"\"\"\n        Get the global pooled embedding for an image.\n        Useful for comparing images or clustering.\n        \n        Args:\n            image: PIL Image or path to image\n            \n        Returns:\n            pooled: [hidden_dim] global representation\n        \"\"\"\n        _, _, pooled = self.compute_attention(image)\n        return pooled\n\n\ndef demo_edge_inference():\n    \"\"\"Demo the edge inference with a sample image.\"\"\"\n    import sys\n    \n    # Check for ONNX export directory\n    onnx_dir = \"./onnx_export\"\n    if not os.path.exists(onnx_dir):\n        print(f\"Error: ONNX export directory not found: {onnx_dir}\")\n        print(\"Please run export_vision_onnx.py first to generate the ONNX model.\")\n        sys.exit(1)\n    \n    # Initialize encoder\n    print(\"Initializing EdgeVisionEncoder...\")\n    encoder = EdgeVisionEncoder(onnx_dir)\n    \n    # Find a test image\n    test_images = [\n        \"ECG-Atrial-Fibrillation-4-1024x561 (1).jpg\",\n        \"normal-sinus-rhythm-2 (1).jpg\",\n        \"ecg_image.png\",\n        \"F1.png\"\n    ]\n    \n    test_image = None\n    for img_name in test_images:\n        if os.path.exists(img_name):\n            test_image = img_name\n            break\n    \n    if test_image is None:\n        # Create a dummy test image\n        test_image = Image.new('RGB', (896, 896), color='white')\n        print(\"Using dummy white image for testing\")\n    else:\n        print(f\"Using test image: {test_image}\")\n    \n    # Compute attention\n    print(\"\\nComputing attention...\")\n    attention_2d, heatmap_image = encoder.get_attention_heatmap(test_image)\n    \n    print(f\"Attention map shape: {attention_2d.shape}\")\n    print(f\"Attention range: [{attention_2d.min():.4f}, {attention_2d.max():.4f}]\")\n    \n    # Get top attention regions\n    top_regions = encoder.get_top_attention_regions(test_image)\n    print(f\"\\nTop 5 attention regions:\")\n    for i, (row, col, score) in enumerate(top_regions):\n        print(f\"  {i+1}. Grid ({row}, {col}): score = {score:.4f}\")\n    \n    # Save heatmap\n    output_path = \"edge_attention_heatmap.png\"\n    heatmap_image.save(output_path)\n    print(f\"\\nHeatmap saved to: {output_path}\")\n    \n    return encoder\n\n\nif __name__ == '__main__':\n    demo_edge_inference()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://upload.wikimedia.org/wikipedia/commons/3/32/ECG_Atrial_Fibrillation.jpg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test it","metadata":{}},{"cell_type":"code","source":"from edge_inference import EdgeVisionEncoder\n\n# Initialize with path to ONNX export folder\nencoder = EdgeVisionEncoder(\"./onnx_export\", model_name=\"vision_encoder_quant.onnx\")\n\n# Get attention heatmap\nattention_2d, heatmap_image = encoder.get_attention_heatmap(\"ECG_Atrial_Fibrillation.jpg\")\nheatmap_image.save(\"attention_overlay.png\")\n\n# Get top attention regions\ntop_regions = encoder.get_top_attention_regions(\"ECG_Atrial_Fibrillation.jpg\", top_k=5)\n\n# Get pooled embedding for similarity comparisons\nembedding = encoder.get_pooled_embedding(\"ECG_Atrial_Fibrillation.jpg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4 Bit Quantization","metadata":{}},{"cell_type":"code","source":"\"\"\"\nQuantize ONNX Vision Encoder to INT8 (4x Smaller)\n==================================================\n\nRun this on your DESKTOP to compress the vision_encoder.onnx model.\n\nUsage:\n    python quantize_onnx.py\n\nThis will create:\n- onnx_export/vision_encoder_quant.onnx (~150MB)\n\"\"\"\n\nimport os\nimport onnx\nfrom onnxruntime.quantization import quantize_dynamic, QuantType\n\ndef quantize_model(input_path, output_path):\n    print(f\"Quantizing {input_path}...\")\n    \n    # Dynamic quantization to INT8 (roughly 4x smaller than FP32)\n    # This works great for vision transformers/ViT/SigLIP on CPU\n    quantize_dynamic(\n        model_input=input_path,\n        model_output=output_path,\n        weight_type=QuantType.QUInt8,  # Quantize weights to UINT8\n    )\n    \n    # Get sizes\n    orig_size = os.path.getsize(input_path) / (1024 * 1024)\n    quant_size = os.path.getsize(output_path) / (1024 * 1024)\n    \n    print(f\"Done!\")\n    print(f\"Original size: {orig_size:.2f} MB\")\n    print(f\"Quantized size: {quant_size:.2f} MB\")\n    print(f\"Reduction: {orig_size / quant_size:.1f}x\")\n\ndef main():\n    onnx_dir = \"./onnx_export\"\n    input_model = os.path.join(onnx_dir, \"vision_encoder.onnx\")\n    output_model = os.path.join(onnx_dir, \"vision_encoder_quant.onnx\")\n    \n    if not os.path.exists(input_model):\n        print(f\"Error: Could not find {input_model}\")\n        print(\"Please make sure you have downloaded the onnx_export folder from Kaggle.\")\n        return\n\n    quantize_model(input_model, output_model)\n    \n    print(\"\\n✅ Quantization complete!\")\n    print(f\"New model saved to: {output_model}\")\n    print(\"\\nTo use this model, update your edge_inference.py call:\")\n    print('encoder = EdgeVisionEncoder(\"./onnx_export\", model_name=\"vision_encoder_quant.onnx\")')\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference the quantized model","metadata":{}},{"cell_type":"code","source":"\"\"\"\nEdge Inference with Quantized ONNX Model (INT8)\n=================================================\n\nDedicated script for running the ~150MB INT8 quantized vision encoder.\nRun this on your desktop app for fast, low-memory inference.\n\nStep 1: Run 'quantize_onnx.py' first to generate the quantized model\nStep 2: Run this script to test it\n\nUsage:\n    from quantized_inference import QuantizedVisionEncoder\n    \n    encoder = QuantizedVisionEncoder(\"./onnx_export\")\n    attention_map, heatmap_image = encoder.get_attention_heatmap(ecg_image)\n\"\"\"\n\nimport os\nimport json\nimport time\nimport numpy as np\nfrom PIL import Image\n\n\nclass QuantizedVisionEncoder:\n    \"\"\"\n    Inference wrapper for the Quantized (INT8) ONNX model.\n    \"\"\"\n    \n    def __init__(self, onnx_dir, model_name=\"vision_encoder_quant.onnx\"):\n        import onnxruntime as ort\n        \n        self.onnx_dir = onnx_dir\n        self.model_path = os.path.join(onnx_dir, model_name)\n        self.config_path = os.path.join(onnx_dir, \"vision_config.json\")\n        \n        # Check if model exists\n        if not os.path.exists(self.model_path):\n            raise FileNotFoundError(f\"Quantized model not found at {self.model_path}. Please run quantize_onnx.py first.\")\n            \n        # Load config\n        with open(self.config_path, 'r') as f:\n            self.config = json.load(f)\n            \n        # Initialize session options for CPU performance\n        sess_options = ort.SessionOptions()\n        sess_options.intra_op_num_threads = 4  # Adjust based on CPU cores\n        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n        \n        print(f\"Loading quantized model: {model_name}...\")\n        start_time = time.time()\n        \n        # Run on CPU (Quantized models are optimized for CPU)\n        self.session = ort.InferenceSession(\n            self.model_path, \n            sess_options, \n            providers=['CPUExecutionProvider']\n        )\n        \n        # Load projection weight if available (for full CMAS)\n        proj_path = os.path.join(onnx_dir, \"projection_weight.npy\")\n        if os.path.exists(proj_path):\n            self.projection_weight = np.load(proj_path)\n        else:\n            self.projection_weight = None\n\n        load_time = time.time() - start_time\n        print(f\"Model loaded in {load_time:.2f}s\")\n        \n        self.input_height = self.config.get('input_height', 896)\n        self.input_width = self.config.get('input_width', 896)\n        self.grid_size = self.config.get('grid_size', 16)\n    \n    def preprocess(self, image):\n        \"\"\"Standard preprocessing for MedGemma vision encoder.\"\"\"\n        if isinstance(image, str):\n            image = Image.open(image)\n        \n        if image.mode != 'RGB':\n            image = image.convert('RGB')\n            \n        # Resize\n        image = image.resize((self.input_width, self.input_height), Image.BILINEAR)\n        \n        # Normalize\n        img = np.array(image).astype(np.float32) / 255.0\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        img = (img - mean) / std\n        \n        # CHW format\n        img = img.transpose(2, 0, 1)\n        img = np.expand_dims(img, 0).astype(np.float32)\n        return img\n\n    def get_attention_heatmap(self, image, alpha=0.5, colormap='jet', invert=True, threshold=0.6):\n        \"\"\"\n        Get attention heatmap overlay.\n        threshold: Only show attention above this value (0-1). \n                   Higher = less red, more focused on peaks.\n        \"\"\"\n        # Preprocess\n        pixel_values = self.preprocess(image)\n        \n        # Inference\n        start_time = time.time()\n        outputs = self.session.run(None, {'pixel_values': pixel_values})\n        infer_time = time.time() - start_time\n        \n        projected, attention, pooled = outputs\n        \n        # Process attention\n        attention_2d = attention[0].reshape(self.grid_size, self.grid_size)\n        \n        # Normalize to 0-1\n        att_min, att_max = attention_2d.min(), attention_2d.max()\n        if att_max > att_min:\n            attention_2d = (attention_2d - att_min) / (att_max - att_min)\n            \n        # Apply thresholding/scaling to focus on peaks\n        # 1. Hard threshold: Zero out anything below threshold\n        # attention_2d[attention_2d < threshold] = 0\n        \n        # 2. Linear Windowing (Robust Control)\n        # vmin: Below this is Blue (0.0) -> Suppress Background\n        # vmax: Above this is Red (1.0) -> Boost faint signals\n        \n        vmin = 0.3  # Increase to suppress more background\n        vmax = 0.7  # Decrease to make yellow/orange appear Red\n        \n        # Apply window\n        attention_2d = (attention_2d - vmin) / (vmax - vmin + 1e-8)\n        \n        # Clip to 0-1 range\n        attention_2d = np.clip(attention_2d, 0, 1)\n            \n        if invert:\n            # For 'jet' colormap: Red is high (1.0), Blue is low (0.0)\n            # Default attention: high value = important\n            # If we want red=important, we don't invert if map is 0..1\n            pass \n        else:\n            attention_2d = 1.0 - attention_2d\n            \n        # Generate overlay\n        heatmap_image = self._overlay_heatmap(image, attention_2d, alpha, colormap)\n        \n        return attention_2d, heatmap_image, infer_time\n\n    def _overlay_heatmap(self, image, attention, alpha, colormap):\n        import matplotlib.cm as cm\n        \n        if isinstance(image, str):\n            image = Image.open(image).convert('RGB')\n        \n        w, h = image.size\n        \n        # Resize attention\n        att_img = Image.fromarray((attention * 255).astype(np.uint8))\n        att_img = att_img.resize((w, h), Image.BILINEAR)\n        att_arr = np.array(att_img) / 255.0\n        \n        # Colorize\n        cmap = cm.get_cmap(colormap)\n        colored = (cmap(att_arr)[:, :, :3] * 255).astype(np.uint8)\n        \n        # Blend\n        img_arr = np.array(image)\n        blended = (1 - alpha) * img_arr + alpha * colored\n        return Image.fromarray(blended.astype(np.uint8))\n\n\ndef demo():\n    onnx_dir = \"./onnx_export\"\n    if not os.path.exists(os.path.join(onnx_dir, \"vision_encoder_quant.onnx\")):\n        print(\"Error: Quantized model not found. Run quantize_onnx.py first!\")\n        return\n\n    encoder = QuantizedVisionEncoder(onnx_dir)\n    \n    # Try to find a test image\n    test_img = \"/kaggle/working/ECG_Atrial_Fibrillation.jpg\"\n    if not os.path.exists(test_img):\n        # Create dummy\n        Image.new('RGB', (500, 300), color='white').save(test_img)\n        print(\"Created dummy test image.\")\n        \n    print(f\"\\nRunning inference on {test_img}...\")\n    att, heatmap, latency = encoder.get_attention_heatmap(test_img)\n    \n    print(f\"Inference time: {latency*1000:.1f} ms\")\n    print(f\"Attention map: {att.shape}\")\n    \n    heatmap.save(\"quantized_heatmap.png\")\n    print(\"Saved quantized_heatmap.png\")\n\nif __name__ == \"__main__\":\n    demo()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T18:55:45.276173Z","iopub.execute_input":"2026-02-07T18:55:45.277062Z","iopub.status.idle":"2026-02-07T18:56:25.185832Z","shell.execute_reply.started":"2026-02-07T18:55:45.277028Z","shell.execute_reply":"2026-02-07T18:56:25.185061Z"}},"outputs":[{"name":"stdout","text":"Loading quantized model: vision_encoder_quant.onnx...\nModel loaded in 0.96s\n\nRunning inference on /kaggle/working/ECG_Atrial_Fibrillation.jpg...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_359/1751890329.py:164: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  cmap = cm.get_cmap(colormap)\n","output_type":"stream"},{"name":"stdout","text":"Inference time: 38496.1 ms\nAttention map: (16, 16)\nSaved quantized_heatmap.png\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}